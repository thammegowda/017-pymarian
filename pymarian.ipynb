{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMarian Demo\n",
    "\n",
    "Introducing python bindigs for Marian NMT.\n",
    "\n",
    "* Source code: https://github.com/marian-nmt/marian-dev/  under `src/python` directory\n",
    "* PyPI: https://pypi.org/project/pymarian/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep pymarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: pymarian-eval [-h] [-m MODEL] [-v VOCAB] [-l {comet-qe,bleurt,comet}]\n",
      "                     [-V] [-] [-t MT_FILE] [-s SRC_FILE] [-r REF_FILE]\n",
      "                     [-f FIELD [FIELD ...]] [-o OUT] [-a {skip,append,only}]\n",
      "                     [-w WIDTH] [--debug] [--fp16] [--mini-batch MINI_BATCH]\n",
      "                     [-d [DEVICES ...] | -c CPU_THREADS] [-ws WORKSPACE] [-pc]\n",
      "                     [--cache CACHE]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -m MODEL, --model MODEL\n",
      "                        Model name, or path. Known models: bleurt-20,\n",
      "                        wmt20-comet-da, wmt20-comet-qe-da, wmt20-comet-qe-\n",
      "                        da-v2, wmt21-comet-da, wmt21-comet-qe-da, wmt21-comet-\n",
      "                        qe-mqm, wmt22-comet-da, wmt22-cometkiwi-da,\n",
      "                        wmt23-cometkiwi-da-xl, wmt23-cometkiwi-da-xxl\n",
      "                        (default: wmt22-cometkiwi-da)\n",
      "  -v VOCAB, --vocab VOCAB\n",
      "                        Vocabulary file (default: None)\n",
      "  -l {comet-qe,bleurt,comet}, --like {comet-qe,bleurt,comet}\n",
      "                        Model type. Required if --model is a local file (auto\n",
      "                        inferred for known models) (default: None)\n",
      "  -V, --version         show program's version number and exit\n",
      "  -, --stdin            Read input from stdin. TSV file with following format:\n",
      "                        QE metrics: \"src<tab>mt\", Ref based metrics ref:\n",
      "                        \"src<tab>mt<tab>ref\" or \"mt<tab>ref\" (default: False)\n",
      "  -t MT_FILE, --mt MT_FILE\n",
      "                        MT output file. Ignored when --stdin (default: None)\n",
      "  -s SRC_FILE, --src SRC_FILE\n",
      "                        Source file. Ignored when --stdin (default: None)\n",
      "  -r REF_FILE, --ref REF_FILE\n",
      "                        Ref file. Ignored when --stdin (default: None)\n",
      "  -f FIELD [FIELD ...], --fields FIELD [FIELD ...]\n",
      "                        Input fields, an ordered sequence of {src, mt, ref}\n",
      "                        (default: ['src', 'mt', 'ref'])\n",
      "  -o OUT, --out OUT     output file (default: <_io.TextIOWrapper\n",
      "                        name='<stdout>' mode='w' encoding='utf-8'>)\n",
      "  -a {skip,append,only}, --average {skip,append,only}\n",
      "                        Average segment scores to produce system score.\n",
      "                        skip=do not output average (default; segment scores\n",
      "                        only); append=append average at the end; only=output\n",
      "                        the average only (i.e. system score only) (default:\n",
      "                        skip)\n",
      "  -w WIDTH, --width WIDTH\n",
      "                        Output score width (default: 4)\n",
      "  --debug               Debug or verbose mode (default: False)\n",
      "  --fp16                Enable FP16 mode (default: False)\n",
      "  --mini-batch MINI_BATCH\n",
      "                        Mini-batch size (default: 16)\n",
      "  -d [DEVICES ...], --devices [DEVICES ...]\n",
      "                        GPU device IDs (default: None)\n",
      "  -c CPU_THREADS, --cpu-threads CPU_THREADS\n",
      "                        Use CPU threads. 0=use GPU device 0 (default: None)\n",
      "  -ws WORKSPACE, --workspace WORKSPACE\n",
      "                        Workspace memory (default: 8000)\n",
      "  -pc, --print-cmd      Print marian evaluate command and exit (default:\n",
      "                        False)\n",
      "  --cache CACHE         Cache directory for storing models (default:\n",
      "                        /mnt/home/tg/.cache/marian/metric)\n",
      "\n",
      "More info at https://github.com/marian-nmt/marian-dev. This CLI is loaded from\n",
      "/mnt/home/tg/mambaforge/envs/xling/lib/python3.10/site-\n",
      "packages/pymarian/eval.py (version: 1.12.31)\n"
     ]
    }
   ],
   "source": [
    "!pymarian-eval -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sacrebleu -t wmt23 -l en-ja --echo ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Config model type is comet-unified, but given: comet-qe\n",
      "INFO: Input field mappings: [0, 1]; expected: ('src', 'mt'), given: ['src', 'mt', 'ref']\n",
      "INFO: CLI:\tmarian evaluate --quiet --model /mnt/home/tg/.cache/marian/metric/models--unbabel--wmt22-cometkiwi-da-marian/snapshots/2082e0fb2e7a3cde527dfd35ebb6dbf6a2e83db6/checkpoints/marian.model.bin --vocabs /mnt/home/tg/.cache/marian/metric/models--microsoft--infoxlm-large/snapshots/d616d637f0720deda963cebbfc630657d2b7d3ae/sentencepiece.bpe.spm /mnt/home/tg/.cache/marian/metric/models--microsoft--infoxlm-large/snapshots/d616d637f0720deda963cebbfc630657d2b7d3ae/sentencepiece.bpe.spm --width 4 --like comet-qe --mini-batch 16 --maxi-batch 256 --max-length 512 --max-length-crop true --workspace 8000 --average skip\n",
      "0.8486\n",
      "0.8494\n",
      "0.8167\n",
      "0.8300\n",
      "0.8715\n",
      "0.8487\n",
      "0.8435\n",
      "0.8735\n",
      "0.9078\n",
      "0.8822\n",
      "0.8538\n",
      "0.8846\n",
      "0.8846\n",
      "0.8842\n",
      "0.8055\n",
      "0.7314\n",
      "0.5996\n",
      "0.8883\n",
      "0.8788\n",
      "0.8065\n",
      "INFO: Wrote 20 lines to <stdout>\n"
     ]
    }
   ],
   "source": [
    "!sacrebleu -t wmt23 -l en-ja --echo src GPT4-5shot | head -20 | pymarian-eval --stdin -m wmt22-cometkiwi-da "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymarian\n",
    "print(f'pymarian {pymarian.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download as hf_get\n",
    "from pathlib import Path\n",
    "from pymarian import Evaluator\n",
    "\n",
    "model_id = \"marian-nmt/chrfoid-wmt23\"\n",
    "model = Path(hf_get(model_id, filename=\"checkpoints/marian.model.bin\"))\n",
    "vocab = Path(hf_get(model_id, filename=\"vocab.spm\"))\n",
    "\n",
    "evaluator = Evaluator.new(\n",
    "    model_file=Path(model), vocab_file=Path(vocab),\n",
    "    like='comet-qe', quiet=True, fp16=False)\n",
    "\n",
    "srcs =  ['Hello', 'Howdy']\n",
    "mts = ['Howdy', 'Hello']\n",
    "lines = [f'{s}\\t{t}' for s,t in zip(srcs, mts)]\n",
    "scores = evaluator.evaluate(lines)\n",
    "for score in scores:\n",
    "    print(f'{score:.4f}')\n",
    "\n",
    "\n",
    "# release the GPU memory\n",
    "del evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Translator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo , Guten Morgen .\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import tarfile\n",
    "from pymarian import Translator\n",
    "\n",
    "model_url = \"http://data.statmt.org/romang/marian-regression-tests/models/wngt19.tar.gz\"\n",
    "model_dir = Path.home() / 'tmp' /  'marian-models'\n",
    "model_file = str(model_dir / 'wngt19' / 'model.base.npz')\n",
    "vocab_file = str(model_dir / 'wngt19' / 'en-de.spm')\n",
    "\n",
    "if not Path(model_file).exists():\n",
    "    print(f\"Downloading {model_url} and extracting to {model_dir}\")\n",
    "    request = urllib.request.urlopen(model_url)\n",
    "    with tarfile.open(fileobj=request, mode=\"r|gz\") as tar:\n",
    "        tar.extractall(path=model_dir)\n",
    "    print(\"Downloaded and extracted model files\")\n",
    "\n",
    "translator = Translator(models=model_file, vocabs=[vocab_file, vocab_file], quiet=True)\n",
    "hyp = translator.translate(\"Hello. Good morning.\")\n",
    "print(hyp)\n",
    "\n",
    "\n",
    "# release the GPUs\n",
    "del translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Trainer\n",
    "\n",
    "### Train an NMT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_url = \"https://textmt.blob.core.windows.net/www/data/marian-tests-data.tgz\"\n",
    "data_dir = Path.home() / 'tmp' / 'marian-tests-data/deu-eng'\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "vocab_file = data_dir / 'vocab.8k.spm'\n",
    "train_src = data_dir / 'sample.5k.deu'\n",
    "train_tgt = train_src.with_suffix('.eng')\n",
    "\n",
    "if not train_tgt.exists():\n",
    "    print(f\"Downloading data package... to {data_dir}\")\n",
    "    with urllib.request.urlopen(data_url) as response:\n",
    "        with tarfile.open(fileobj=response, mode=\"r|gz\") as tar:\n",
    "            tar.extractall(path=data_dir.parent.parent)\n",
    "    print(\"Downloaded the data package\")\n",
    "\n",
    "!head -n4 {train_src} {train_tgt}\n",
    "\n",
    "vocab_file = str(vocab_file)\n",
    "train_src = str(train_src)\n",
    "train_tgt = str(train_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymarian import Trainer\n",
    "args = {\n",
    "    'type': 'transformer',\n",
    "    'dim_emb': 512,\n",
    "    'after': '1000u',  # stop after 500 updates\n",
    "    'valid_freq': '1000u',  # validate every 250 updates\n",
    "    'disp_freq': 100,\n",
    "    'disp_first': 4,\n",
    "    'save_freq': '100u',\n",
    "    'vocabs': [vocab_file, vocab_file],\n",
    "    'train_sets': [train_src, train_tgt],\n",
    "    'devices' :  'all',\n",
    "    'quiet': False,\n",
    "}\n",
    "\n",
    "model_file = args['model'] = f'{data_dir.parent}/model.npz'\n",
    "\n",
    "\n",
    "trainer = Trainer(**args)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "!ls -lh {model_file}\n",
    "\n",
    "# release the GPUs\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an Evaluator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = data_dir / 'vocab.8k.spm'\n",
    "classes_file = data_dir / 'classes4f.txt'\n",
    "train_file = data_dir / 'sample.5k.chrfoid-deu-eng.tsv'\n",
    "\n",
    "assert classes_file.exists()\n",
    "assert vocab_file.exists()\n",
    "assert train_file.exists()\n",
    "\n",
    "!head -n4 {train_file} {classes_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args = {\n",
    "    'dim_emb': 512,\n",
    "    'enc_depth': 6,\n",
    "    'dec_depth': 6,\n",
    "    'tied_embeddings_all': True,\n",
    "    'transformer_heads': 2,\n",
    "    'transformer_dim_ffn': 256,\n",
    "    'transformer_ffn_activation': 'relu',\n",
    "    'transformer_dropout': 0.1,\n",
    "    'cost_type': 'ce-mean',\n",
    "    'max_length': 80,\n",
    "    'mini_batch_fit': False,\n",
    "    'maxi_batch': 256,\n",
    "    'optimizer_params': [0.9, 0.98, '1e-09'],\n",
    "    'sync_sgd': True,\n",
    "    'learn_rate': 0.0003,\n",
    "    'lr_decay_inv_sqrt': [16000],\n",
    "    'lr_warmup': 16000,\n",
    "    'label_smoothing': 0.1,\n",
    "    'clip_norm': 0,\n",
    "    'exponential_smoothing': 0.0001,\n",
    "    'early_stopping': 2,\n",
    "    'keep_best': True,\n",
    "    'beam_size': 2,\n",
    "    'normalize': 1,\n",
    "    'valid_metrics': ['perplexity'],\n",
    "    'valid_mini_batch': 16,\n",
    "    'mini_batch': 8,\n",
    "    'after': '400u',\n",
    "    'valid_freq': '200u',\n",
    "    'disp_freq': 100,\n",
    "    'disp_first': 4,\n",
    "    'save_freq': '200u',\n",
    "    'quiet': False,\n",
    "    #'like': 'comet-qe',   # only supported at inference; for training, see task and input_types\n",
    "    'task': 'comet-qe',\n",
    "    'input_types': ['class', 'sequence', 'sequence'],  # required for training\n",
    "    #'pretrained_model': pretrained_model,     # for finetuning; not using it because its too big for tests\n",
    "    'train_sets': [train_file],  # TSV file having 3 columns: class sequence sequence\n",
    "    'tsv': True,\n",
    "    'tsv-fields': 3,  # or it will complain that vocabs and train_sets should be one to one map\n",
    "    'vocabs': [classes_file, vocab_file, vocab_file],  # class sequence sequence\n",
    "}\n",
    "\n",
    "save_at = str(data_dir.parent / 'runs/eval.model.npz')\n",
    "trainer = Trainer(model=save_at, **args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
